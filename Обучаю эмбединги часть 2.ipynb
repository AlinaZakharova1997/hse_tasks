{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "# эти штуки нужны для ворд ту века, чтобы обучить модель на вики дате, вики дату надо запихать в эти функции\n",
    "# а потом надо взять данные и их тоже запихать и тоооолько потом уже пытаться делать векторы и что то там себе оценивать!!!\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [token.text for token in list(razdel_tokenize(text))]\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# вот эта штука нужна для уже предобученной модели русвек, прежде чем пихать в нее данные,\n",
    "# их надо запихать в функцию normalize_mystem()\n",
    "mapping = {}\n",
    "\n",
    "for line in open('ru-rnc.map.txt'):\n",
    "    ms, ud = line.strip('\\n').split()\n",
    "    mapping[ms] = ud\n",
    "    \n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "def normalize_mystem(text):\n",
    "    tokens = []\n",
    "    norm_words = m.analyze(text)\n",
    "    for norm_word in norm_words:\n",
    "        if 'analysis' not in norm_word:\n",
    "            continue\n",
    "            \n",
    "        if not len(norm_word['analysis']):\n",
    "            lemma = norm_word['text']\n",
    "            pos = 'UNKN'\n",
    "        else:\n",
    "            lemma = norm_word[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = norm_word[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "        pos = mapping[pos]\n",
    "        tokens.append(lemma+'_'+pos)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# тут будут лежать списки с близостями для каждой метрики\n",
    "all_similarities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), \\nFastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете.\\nМежду векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары). \\nПостройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё)\\nна этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру). \\nПопробуйте улучить метрику, изменив параметры в методах векторизации.\\n!!УТОЧНЕНИЕ: модель нужно обучить сразу на всех 5 близостях, а не по 1 модели на каждой близости!\\nSVD и NMF применяйте к данным напрямую, а w2w и fastext обучите на отдельном корпусе (как в первой части). '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Преобразуйте тексты в векторы в каждой паре 5 методами  - SVD, NMF, Word2Vec (свой и  русвекторовский), \n",
    "Fastext. У вас должно получиться 5 пар векторов для каждой строчки в датасете.\n",
    "Между векторами каждой пары вычислите косинусную близость (получится 5 чисел для каждой пары). \n",
    "Постройте обучающую выборку из этих близостей . Обучите любую модель (Логрег, Рандом форест или что-то ещё)\n",
    "на этой выборке и оцените качество на кросс-валидации (используйте микросреднюю f1-меру). \n",
    "Попробуйте улучить метрику, изменив параметры в методах векторизации.\n",
    "!!УТОЧНЕНИЕ: модель нужно обучить сразу на всех 5 близостях, а не по 1 модели на каждой близости!\n",
    "SVD и NMF применяйте к данным напрямую, а w2w и fastext обучите на отдельном корпусе (как в первой части). '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут я буду обучать модель самостоятельно word2vec, не на парафразах! это точно не они\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "# Download dataset\n",
    "\n",
    "file = open('wiki_data.txt').read().splitlines()\n",
    "data_norm = [normalize(text) for text in file]\n",
    "data_norm = [text for text in data_norm if text]\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "# используй все ядра и отсеки слова с частотой 1\n",
    "#model = Word2Vec([text.split() for text in data_norm], min_count = 10, workers=-1)\n",
    "# FASTTEXT MODEL ON LEARNING\n",
    "# fasttext = gensim.models.FastText([text.split() for text in data_norm], size=50, min_n=10, max_n=8) \n",
    "# Save and Load Model\n",
    "# model.save('newmodel')\n",
    "# model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь надо взять тексты, открываем парафразы\n",
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "data['text_1_norm'] = data['text_1'].apply(normalize).apply(tokenize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize).apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model, dim):\n",
    "    \n",
    "    #text = text.split()\n",
    "    #print(text)\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in model:\n",
    "            new_text.append(word)\n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(new_text)\n",
    "    #print(words)\n",
    "    total = len(new_text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        v = model[word]\n",
    "        #print(v,'vvvvvv')\n",
    "        vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "        #print(vectors[i])\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        #print('else')\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "/home/alina/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# делаем для ворд ту века\n",
    "# это размерность вектора\n",
    "dim = 100\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "#print(model.wv.index2word)\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, model, dim=100)\n",
    "    #print( X_text_1_w2v[i],'X_text_1_w2v[i]')\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, model, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cousine_sim_vectors(vec1, vec2):\n",
    "    vec1 = vec1.reshape(1, -1)\n",
    "    vec2 = vec2.reshape(1, -1)\n",
    "    return cosine_similarity(vec1, vec2)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8783001020339976]\n"
     ]
    }
   ],
   "source": [
    "similarities = {}\n",
    "for i in range(len( X_text_1_w2v)):\n",
    "    value = [cousine_sim_vectors(X_text_1_w2v[i], X_text_2_w2v[i])]\n",
    "    similarities.setdefault(i,[]).extend(value)\n",
    "print(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUSVEC\n",
    "rusvec = gensim.models.KeyedVectors.load_word2vec_format('180/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_1_norm'] = data['text_1'].apply(normalize_mystem)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize_mystem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300\n",
    "X_text_1_rusvec = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_rusvec = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_rusvec[i] = get_embedding(text, rusvec, dim=300)\n",
    "    #print(X_text_1_rusvec[i])\n",
    "    \n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_rusvec[i] = get_embedding(text, rusvec, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8783001020339976, 0.6844317556056889]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_text_1_rusvec)):\n",
    "    value = [cousine_sim_vectors(X_text_1_rusvec[i], X_text_2_rusvec[i])]\n",
    "    similarities.setdefault(i,[]).extend(value)\n",
    "print(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTTEXT\n",
    "data['text_1_norm'] = data['text_1'].apply(normalize).apply(tokenize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize).apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/.local/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  import sys\n",
      "/home/alina/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "dim = 100\n",
    "X_text_1_fast = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_fast = np.zeros((len(data['text_2_norm']), dim))\n",
    "#print(model.wv.index2word)\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_fast[i] = get_embedding(text, model, dim=100)\n",
    "    #print( X_text_1_w2v[i],'X_text_1_w2v[i]')\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_fast[i] = get_embedding(text, model, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8783001020339976, 0.6844317556056889, 0.8783001020339976]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_text_1_fast)):\n",
    "    value = [cousine_sim_vectors(X_text_1_fast[i], X_text_2_fast[i])]\n",
    "    similarities.setdefault(i,[]).extend(value)\n",
    "print(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD\n",
    "svd = TruncatedSVD(200)\n",
    "tfidf = TfidfVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "tfidf.fit(pd.concat([data['text_1_norm'], data['text_2_norm']]))\n",
    "X_text_1 = svd.fit_transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2 = svd.fit_transform(tfidf.transform(data['text_2_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8783001020339976, 0.6844317556056889, 0.8783001020339976, 0.07258040517513242]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_text_2)):\n",
    "    value = [cousine_sim_vectors(X_text_1[i], X_text_2[i])]\n",
    "    similarities.setdefault(i,[]).extend(value)\n",
    "print(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/.local/lib/python3.6/site-packages/sklearn/decomposition/_nmf.py:1077: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# NMF\n",
    "cv = CountVectorizer(min_df=3, max_df=0.4, max_features=1000)\n",
    "X = cv.fit_transform(data_norm)\n",
    "nmf = NMF(50)\n",
    "nmf.fit(X)\n",
    "X_text_1_nmf = nmf.transform(tfidf.transform(data['text_1_norm']))\n",
    "X_text_2_nmf = nmf.transform(tfidf.transform(data['text_2_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8783001020339976, 0.6844317556056889, 0.8783001020339976, 0.07258040517513242, 0.41675942861600385]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_text_1_nmf)):\n",
    "    value = [cousine_sim_vectors(X_text_1_nmf[i], X_text_2_nmf[i])]\n",
    "    similarities.setdefault(i,[]).extend(value)\n",
    "print(similarities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8783001   0.68443176  0.8783001   0.07258041  0.41675943]\n",
      " [ 0.93955153  0.7547428   0.93955153 -0.00107577  0.4892001 ]\n",
      " [ 0.9646989   0.8264399   0.9646989  -0.01766951  0.85280206]\n",
      " ...\n",
      " [ 0.8952867   0.52718837  0.8952867  -0.06267831  0.94976582]\n",
      " [ 0.88573977  0.44182585  0.88573977 -0.05209853  0.77623609]\n",
      " [ 0.6808095   0.53620972  0.6808095   0.00955381  0.07738828]]\n"
     ]
    }
   ],
   "source": [
    "# делаем матрицу, чтобы запихать ее в форест\n",
    "all_similarities = []\n",
    "for value in similarities.values():\n",
    "    all_similarities.append(value)\n",
    "matrix_for_forest = np.array(all_similarities)\n",
    "print(matrix_for_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.65      0.59      0.62       629\n",
      "           0       0.50      0.53      0.51       737\n",
      "           1       0.50      0.50      0.50       441\n",
      "\n",
      "    accuracy                           0.55      1807\n",
      "   macro avg       0.55      0.54      0.54      1807\n",
      "weighted avg       0.55      0.55      0.55      1807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучаем форест\n",
    "y = data['label'].values\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(matrix_for_forest, y,random_state=1)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=7, min_samples_leaf=15,\n",
    "                             class_weight='balanced')\n",
    "clf.fit(train_X, train_y)\n",
    "preds = clf.predict(valid_X)\n",
    "print(classification_report(valid_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55 accuracy with a standard deviation of 0.01\n"
     ]
    }
   ],
   "source": [
    "# оцениваем форест на кросс валидации\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(clf, train_X, train_y, cv=5)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
