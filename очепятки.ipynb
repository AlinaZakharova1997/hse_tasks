{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "Collecting scikit-learn (from sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
      "Collecting scipy>=0.19.1 (from scikit-learn->sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting numpy>=1.13.3 (from scikit-learn->sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/fc/36e52d0ae2aa502b211f1bcd2fdeec72d343d58224eabcdddc1bcb052db1/numpy-1.19.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting joblib>=0.11 (from scikit-learn->sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/fc/c9/f58220ac44a1592f79a343caba12f6837f9e0c04c196176a3d66338e1ea8/joblib-0.17.0-py3-none-any.whl\n",
      "Installing collected packages: threadpoolctl, numpy, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.17.0 numpy-1.19.4 scikit-learn-0.23.2 scipy-1.5.4 sklearn-0.0 threadpoolctl-2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "!pip3 install sklearn\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "!pip3 install textdistance\n",
    "import textdistance\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text)\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('Рабочий стол/sents_with_mistakes.txt', encoding='utf8').read().splitlines()[:100]\n",
    "true = open('Рабочий стол/correct_sents.txt', encoding='utf8').read().splitlines()[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad[9])\n",
    "print(true[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = []\n",
    "total = 0\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if pair[0] != pair[1]:\n",
    "            mistakes.append(pair)\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ошибки вычисляются также, по словарю. Но ещё мы собираем частотности, \n",
    "# чтобы потом выбирать самое вероятное исправление.\n",
    "# реализуем алгоритм Symspell, который как Норвиг, но другой\n",
    "# Нас интересует всего один вид ошибки - удаление\n",
    "\n",
    "corpus = open('Рабочий стол/wiki_data.txt', encoding='utf8').read()[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем словарь из корпуса, в котором будем потом искать слова\n",
    "WORDS = Counter(re.findall('\\w+', corpus.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фунцкия расчета вероятности появления слова в корпусе\n",
    "N = sum(WORDS.values())\n",
    "def P(word, N=N): \n",
    "    \"Вычисляем вероятность слова\"\n",
    "    return WORDS[word] / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P('апофеоз')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы найти исправления нужно сгенерировать возможные исправления и выбрать те, \n",
    "# которые есть в словаре. Если есть несколько вариантов, то выбрать тот, у котогоро наибольшая вероятность.\n",
    "\n",
    "def correction(word): \n",
    "    \"Находим наиболее вероятное похожее слово\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Генерируем кандидатов на исправление\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"Выбираем слова, которые есть в корпусе\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
    "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    return deletes\n",
    "\n",
    "def edits2(word): \n",
    "    \"Создаем кандидатов, которые отличаются на две буквы\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'сонце'\n",
    "splits = [(word[:i], word[i:])    for i in range(len(word) + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletes = [L + R[1:] for L, R in splits if R]\n",
    "deletes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edits1('девочка'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(edits2('девочка')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для оценки используем будем использовать три метрики:\n",
    "\n",
    "# 1) процент правильных слов;\n",
    "# 2) процент исправленных ошибок\n",
    "# 3) процент ошибочно исправленных правильных слов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "        # перед тем как считать исправление проверим нет ли его в кеше\n",
    "        predicted = cashed.get(pair[1], correction(pair[1]))\n",
    "        cashed[pair[0]] = predicted\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)\n",
    "# Это было у Норвига, у нас пока не сильно прикольнее\n",
    "# 0.7287712287712288\n",
    "# 0.5057559478127398\n",
    "# 0.23785459974732973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте делать триграммную модель, берем наш корпус\n",
    "corpus_wiki = [['<start>', '<start>'] + sent + ['<end>'] for sent in preprocess(corpus)]\n",
    "# функция, которая делает n-граммы\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for sentence in corpus_wiki:\n",
    "    unigrams.update(sentence)\n",
    "    bigrams.update(ngrammer(sentence,2))\n",
    "    trigrams.update(ngrammer(sentence,3))\n",
    "    \n",
    "# эта функция предсказатель ошибок    \n",
    "def predict_mistaken(word, vocab):\n",
    "\n",
    "    if word in vocab:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "# Напишем функцию, которая принимает слово\n",
    "# и находит ближайшее к нему в словаре с помощью косисного расстояния\n",
    "# и честной метрики редактирования.   \n",
    "def get_closest_hybrid_match(text, X, vec, topn=5, metric=textdistance.damerau_levenshtein):\n",
    "    # ваш код здесь\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    sims = Counter()\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup,topn, metric=metric)\n",
    "    return closest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оцените качество также как и раньше\n",
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    \n",
    "    word_pairs = [('<start>', '<start>')] + word_pairs\n",
    "    pred_sent = []\n",
    "    for j in range(1, len(word_pairs)):\n",
    "        \n",
    "        pred = None\n",
    "        \n",
    "        # проверяем, что слова нет в словаре, чтобы не исправлять все слова\n",
    "        if not predict_mistaken(word_pairs[j][1], WORDS):\n",
    "            pred = word_pairs[j][1]\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            # находим кандидатов для исправления\n",
    "            predicted = get_closest_hybrid_match(word_pairs[j][1], X, vec)\n",
    "        \n",
    "            # берем предыдущее слово для контекста\n",
    "            prev_word = word_pairs[j-1][1]\n",
    "            # это предыдущее предыдущего для триграммы\n",
    "            prev_prev_word = word_pairs[j-2][0]\n",
    "        \n",
    "            # если у нас нет в модели такого слова, то мы не сможем вероятность посчитать\n",
    "            # остается только взять первое по близости\n",
    "            if prev_word not in unigrams:\n",
    "                pred = predicted[0][0]\n",
    "            # ?\n",
    "            if prev_prev_word not in unigrams:\n",
    "                pred = predicted[0][0]\n",
    "            \n",
    "        \n",
    "            else:\n",
    "                lm_predicted = []\n",
    "                for word, m in predicted:\n",
    "                    bigram = ' '.join([prev_word, word])\n",
    "                    trigram = ' '.join([prev_prev_word, prev_word, word])\n",
    "                    lm_predicted.append((word, (m)*((trigrams[trigram]/bigrams[bigram]))))\n",
    "\n",
    "                \n",
    "                \n",
    "                if lm_predicted:\n",
    "\n",
    "                    pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
    "\n",
    "        \n",
    "        if pred is None:\n",
    "            pred = word_pairs[j][1]\n",
    "        \n",
    "\n",
    "        \n",
    "        if pred == word_pairs[j][0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if word_pairs[j][0] == word_pairs[j][1]:\n",
    "            total_correct += 1\n",
    "            if word_pairs[j][0] !=  pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if word_pairs[j][0] == pred:\n",
    "                mistaken_fixed += 1\n",
    "    \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
