{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lxml import html\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter,defaultdict\n",
    "from string import punctuation\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "%matplotlib inline\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "# эти штуки нужны для ворд ту века, чтобы обучить модель на вики дате, вики дату надо запихать в эти функции\n",
    "# а потом надо взять данные и их тоже запихать и тоооолько потом уже пытаться делать векторы и что то там себе оценивать!!!\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word and word not in stops]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = [token.text for token in list(razdel_tokenize(text))]\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# вот эта штука нужна для уже предобученной модели русвек, прежде чем пихать в нее данные,\n",
    "# их надо запихать в функцию normalize_mystem()\n",
    "mapping = {}\n",
    "\n",
    "for line in open('ru-rnc.map.txt'):\n",
    "    ms, ud = line.strip('\\n').split()\n",
    "    mapping[ms] = ud\n",
    "    \n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "def normalize_mystem(text):\n",
    "    tokens = []\n",
    "    norm_words = m.analyze(text)\n",
    "    for norm_word in norm_words:\n",
    "        if 'analysis' not in norm_word:\n",
    "            continue\n",
    "            \n",
    "        if not len(norm_word['analysis']):\n",
    "            lemma = norm_word['text']\n",
    "            pos = 'UNKN'\n",
    "        else:\n",
    "            lemma = norm_word[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "            pos = norm_word[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "            pos = pos.split('=')[0].strip()\n",
    "        pos = mapping[pos]\n",
    "        tokens.append(lemma+'_'+pos)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тут я буду обучать модель самостоятельно word2vec, не на парафразах! это точно не они\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "# Download dataset\n",
    "\n",
    "file = open('wiki_data.txt').read().splitlines()\n",
    "data_norm = [normalize(text) for text in file]\n",
    "data_norm = [text for text in data_norm if text]\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "# используй все ядра и отсеки слова с частотой 1\n",
    "model = Word2Vec([text.split() for text in data_norm], min_count = 10, workers=-1)\n",
    "# Save and Load Model\n",
    "# model.save('newmodel')\n",
    "# model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gensim\n",
    "# берем какую то вторую модель, которую я скачала по ссылке из домашки\n",
    "#rusvec = gensim.models.KeyedVectors.load_word2vec_format('180/model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# теперь надо взять тексты, открываем парафразы\n",
    "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
    "texts_1 = []\n",
    "texts_2 = []\n",
    "classes = []\n",
    "\n",
    "for p in corpus_xml.xpath('//paraphrase'):\n",
    "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
    "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
    "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
    "    \n",
    "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data['text_1_norm'] = data['text_1'].apply(normalize_mystem)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize_mystem)'''# rusvec\n",
    "# word2vec\n",
    "data['text_1_norm'] = data['text_1'].apply(normalize).apply(tokenize)\n",
    "data['text_2_norm'] = data['text_2'].apply(normalize).apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''text_test = data['text_1_norm'].values[0]\n",
    "text_test'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def get_embedding(text, model, dim):\n",
    "    text = text.split()\n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(text)\n",
    "    total = len(text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        try:\n",
    "            v = model[word]\n",
    "            vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "            print(vectors[i])\n",
    "        except (KeyError, ValueError):\n",
    "            print('Error')\n",
    "            continue\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector'''\n",
    "\n",
    "def get_embedding(text, model, dim):\n",
    "    \n",
    "    #text = text.split()\n",
    "    #print(text)\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word in model:\n",
    "            new_text.append(word)\n",
    "    # чтобы не доставать одно слово несколько раз\n",
    "    # сделаем счетчик, а потом векторы домножим на частоту\n",
    "    words = Counter(new_text)\n",
    "    #print(words)\n",
    "    total = len(new_text)\n",
    "    vectors = np.zeros((len(words), dim))\n",
    "    \n",
    "    for i,word in enumerate(words):\n",
    "        v = model[word]\n",
    "        #print(v,'vvvvvv')\n",
    "        vectors[i] = v*(words[word]/total) # просто умножаем вектор на частоту\n",
    "        #print(vectors[i])\n",
    "    \n",
    "    if vectors.any():\n",
    "        vector = np.average(vectors, axis=0)\n",
    "    else:\n",
    "        #print('else')\n",
    "        vector = np.zeros((dim))\n",
    "    \n",
    "    return vector\n",
    "\n",
    "#get_embedding(text_test, model, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alina/.local/lib/python3.6/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/home/alina/.local/lib/python3.6/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# делаем для ворд ту века\n",
    "# это размерность вектора\n",
    "dim = 100\n",
    "X_text_1_w2v = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_w2v = np.zeros((len(data['text_2_norm']), dim))\n",
    "#print(model.wv.index2word)\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_w2v[i] = get_embedding(text, model, dim=100)\n",
    "    #print( X_text_1_w2v[i],'X_text_1_w2v[i]')\n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_w2v[i] = get_embedding(text, model, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)\\ny = data['label'].values\\ntrain_X, valid_X, train_y, valid_y = train_test_split(X_text_w2v, y,random_state=1)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''X_text_w2v = np.concatenate([X_text_1_w2v, X_text_2_w2v], axis=1)\n",
    "y = data['label'].values\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text_w2v, y,random_state=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4095941 0.4095941 0.4095941 0.4095941 0.4095941]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, train_X, train_y, cv=5)\n",
    "print(scores)\n",
    "#array([0.96..., 1. , 0.96..., 0.96..., 1. ]) что то примерно такое должно получится"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41 accuracy with a standard deviation of 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dim = 300\\nX_text_1_rusvec = np.zeros((len(data['text_1_norm']), dim))\\nX_text_2_rusvec = np.zeros((len(data['text_2_norm']), dim))\\n\\nfor i, text in enumerate(data['text_1_norm'].values):\\n    X_text_1_rusvec[i] = get_embedding(text, rusvec, dim=300)\\n    print(X_text_1_rusvec[i])\\n    \\n    \\nfor i, text in enumerate(data['text_2_norm'].values):\\n    X_text_2_rusvec[i] = get_embedding(text, rusvec, dim=300)\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# теперь делаем для второй модели\n",
    "'''dim = 300\n",
    "X_text_1_rusvec = np.zeros((len(data['text_1_norm']), dim))\n",
    "X_text_2_rusvec = np.zeros((len(data['text_2_norm']), dim))\n",
    "\n",
    "for i, text in enumerate(data['text_1_norm'].values):\n",
    "    X_text_1_rusvec[i] = get_embedding(text, rusvec, dim=300)\n",
    "    print(X_text_1_rusvec[i])\n",
    "    \n",
    "    \n",
    "for i, text in enumerate(data['text_2_norm'].values):\n",
    "    X_text_2_rusvec[i] = get_embedding(text, rusvec, dim=300)'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45 accuracy with a standard deviation of 0.01\n"
     ]
    }
   ],
   "source": [
    "'''from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "y = data['label'].values\n",
    "X_text_rusvec = np.concatenate([X_text_1_rusvec, X_text_2_rusvec], axis=1)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(X_text_rusvec, y,random_state=1)\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "scores = cross_val_score(clf, train_X, train_y, cv=5)\n",
    "scores\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
